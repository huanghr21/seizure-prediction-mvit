"""
è®­ç»ƒè„šæœ¬
ä¸»è®­ç»ƒæµç¨‹ï¼šæ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œç»“æœä¿å­˜
"""

import os
import sys
import time

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import config
from data.preprocessor import prepare_data
from model.mvit import MultiChannelViT, create_model
from utils import (
    AverageMeter,
    EarlyStopping,
    compute_metrics,
    plot_confusion_matrix,
    plot_training_curves,
    print_metrics,
    save_checkpoint,
    save_results,
    set_seed,
)


def train_epoch(model, dataloader, criterion, optimizer, device):
    """
    è®­ç»ƒä¸€ä¸ªepoch

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡

    Returns:
        avg_loss, avg_acc
    """
    model.train()

    losses = AverageMeter()
    accuracies = AverageMeter()

    pbar = tqdm(dataloader, desc="Training")

    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        # è®¡ç®—å‡†ç¡®ç‡
        pred = output.argmax(dim=1)
        acc = (pred == target).float().mean()

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        losses.update(loss.item(), data.size(0))
        accuracies.update(acc.item(), data.size(0))

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({"loss": f"{losses.avg:.4f}", "acc": f"{accuracies.avg:.4f}"})

    return losses.avg, accuracies.avg


def evaluate(model, dataloader, criterion, device):
    """
    è¯„ä¼°æ¨¡å‹

    Args:
        model: æ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡

    Returns:
        avg_loss, avg_acc, y_true, y_pred
    """
    model.eval()

    losses = AverageMeter()
    accuracies = AverageMeter()

    all_targets = []
    all_preds = []

    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Evaluating")

        for data, target in pbar:
            data, target = data.to(device), target.to(device)

            # å‰å‘ä¼ æ’­
            output = model(data)
            loss = criterion(output, target)

            # è®¡ç®—å‡†ç¡®ç‡
            pred = output.argmax(dim=1)
            acc = (pred == target).float().mean()

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            losses.update(loss.item(), data.size(0))
            accuracies.update(acc.item(), data.size(0))

            # æ”¶é›†é¢„æµ‹ç»“æœ
            all_targets.extend(target.cpu().numpy())
            all_preds.extend(pred.cpu().numpy())

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix(
                {"loss": f"{losses.avg:.4f}", "acc": f"{accuracies.avg:.4f}"}
            )

    return losses.avg, accuracies.avg, np.array(all_targets), np.array(all_preds)


def train_model(
    model,
    train_loader,
    val_loader,
    criterion,
    optimizer,
    scheduler,
    device,
    num_epochs,
    save_dir,
):
    """
    å®Œæ•´çš„è®­ç»ƒæµç¨‹

    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºæ—©åœå’Œé€‰æ‹©æœ€ä½³æ¨¡å‹ï¼‰
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        device: è®¾å¤‡
        num_epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•

    Returns:
        best_metrics: æœ€ä½³æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡
    """
    print("\n" + "=" * 50)
    print("Starting training...")
    print("=" * 50)

    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []

    best_acc = 0.0
    best_metrics = None

    # æ—©åœ
    if config.USE_EARLY_STOPPING:
        early_stopping = EarlyStopping(
            patience=config.EARLY_STOPPING_PATIENCE, verbose=True
        )

    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        print(f"\nEpoch [{epoch + 1}/{num_epochs}]")
        print("-" * 50)

        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device
        )

        # è¯„ä¼°ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰
        val_loss, val_acc, y_true, y_pred = evaluate(
            model, val_loader, criterion, device
        )

        # è®°å½•å†å²
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_losses.append(val_loss)
        val_accs.append(val_acc)

        # æ‰“å°ç»“æœ
        print(f"\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]["lr"]
            print(f"Learning Rate: {current_lr:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            best_metrics = compute_metrics(y_true, y_pred)

            if config.SAVE_BEST_ONLY:
                checkpoint_path = os.path.join(
                    save_dir, f"best_model_{config.SUBJECT}.pth"
                )
                save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_path)
                print(f"âœ“ Best model saved! Accuracy: {best_acc:.4f}")

        # æ—©åœæ£€æŸ¥
        if config.USE_EARLY_STOPPING:
            early_stopping(val_loss)
            if early_stopping.early_stop:
                print("\nEarly stopping triggered!")
                break

    print("\n" + "=" * 50)
    print("Training completed!")
    print("=" * 50)

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    curves_path = os.path.join(save_dir, f"training_curves_{config.SUBJECT}.png")
    history = {
        'train_loss': train_losses,
        'train_acc': train_accs,
        'val_loss': val_losses,
        'val_acc': val_accs
    }
    plot_training_curves(history, curves_path)

    return best_metrics, train_losses, val_losses, train_accs, val_accs


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("Seizure Prediction with Multi-Channel ViT")
    print("=" * 50)
    print(f"Subject: {config.SUBJECT}")
    print(f"Device: {config.DEVICE}")
    print(f"Batch Size: {config.BATCH_SIZE}")
    print(f"Learning Rate: {config.LEARNING_RATE}")
    print(f"Epochs: {config.NUM_EPOCHS}")
    print("=" * 50)

    # è®¾ç½®éšæœºç§å­
    set_seed(config.RANDOM_SEED)

    # 1. å‡†å¤‡æ•°æ®
    print("\n[1/5] Preparing data...")
    train_dataset, val_dataset, test_dataset = prepare_data(config.SUBJECT_PATH)

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,  # Windowsä¸Šå»ºè®®è®¾ç½®ä¸º0
        pin_memory=True if config.DEVICE.type == "cuda" else False,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True if config.DEVICE.type == "cuda" else False,
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True if config.DEVICE.type == "cuda" else False,
    )

    print(f"Train batches: {len(train_loader)}")
    print(f"Val batches: {len(val_loader)}")
    print(f"Test batches: {len(test_loader)}")

    # 2. åˆ›å»ºæ¨¡å‹
    print("\n[2/5] Creating model...")
    model = create_model()
    model = model.to(config.DEVICE)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    print("\n[3/5] Setting up training...")
    criterion = nn.CrossEntropyLoss()

    if config.OPTIMIZER == "Adam":
        optimizer = optim.Adam(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
        )
    else:
        optimizer = optim.SGD(
            model.parameters(),
            lr=config.LEARNING_RATE,
            momentum=0.9,
            weight_decay=config.WEIGHT_DECAY,
        )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if config.USE_SCHEDULER:
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode="min",
            factor=config.SCHEDULER_FACTOR,
            patience=config.SCHEDULER_PATIENCE,
        )

    # 4. è®­ç»ƒæ¨¡å‹
    print("\n[4/5] Training model...")
    start_time = time.time()

    best_metrics, train_losses, val_losses, train_accs, val_accs = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=config.DEVICE,
        num_epochs=config.NUM_EPOCHS,
        save_dir=config.CHECKPOINT_DIR,
    )

    training_time = time.time() - start_time
    print(f"\nTotal training time: {training_time / 60:.2f} minutes")

    # 5. æœ€ç»ˆè¯„ä¼°
    print("\n[5/5] Final evaluation...")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint_path = os.path.join(
        config.CHECKPOINT_DIR, f"best_model_{config.SUBJECT}.pth"
    )
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint["model_state_dict"])
        print(f"Loaded best model from epoch {checkpoint['epoch'] + 1}")

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    _, _, y_true, y_pred = evaluate(model, test_loader, criterion, config.DEVICE)
    final_metrics = compute_metrics(y_true, y_pred)

    # æ‰“å°è¯„ä¼°æŒ‡æ ‡
    print_metrics(final_metrics, title=f"Final Test Results - {config.SUBJECT}")

    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm_path = os.path.join(config.RESULTS_DIR, f"confusion_matrix_{config.SUBJECT}.png")
    plot_confusion_matrix(y_true, y_pred, cm_path)

    # 6. ä¿å­˜ç»“æœ
    results = {
        "subject": config.SUBJECT,
        "model": "MultiChannelViT",
        "config": {
            "n_channels": config.N_CHANNELS,
            "window_size": config.WINDOW_SIZE,
            "filter_range": f"{config.FILTER_LOW}-{config.FILTER_HIGH} Hz",
            "sop": f"{config.SOP / 60} min",
            "sph": f"{config.SPH / 60} min",
            "batch_size": config.BATCH_SIZE,
            "learning_rate": config.LEARNING_RATE,
            "num_epochs": config.NUM_EPOCHS,
            "num_layers": config.NUM_LAYERS,
            "embed_dim": config.EMBED_DIM,
            "num_heads": config.NUM_HEADS,
        },
        "data": {
            "train_samples": len(train_dataset),
            "val_samples": len(val_dataset),
            "test_samples": len(test_dataset),
            "train_preictal": int(train_dataset.labels.sum().item()),
            "train_interictal": len(train_dataset)
            - int(train_dataset.labels.sum().item()),
            "val_preictal": int(val_dataset.labels.sum().item()),
            "val_interictal": len(val_dataset) - int(val_dataset.labels.sum().item()),
            "test_preictal": int(test_dataset.labels.sum().item()),
            "test_interictal": len(test_dataset)
            - int(test_dataset.labels.sum().item()),
        },
        "training": {
            "total_time_minutes": round(training_time / 60, 2),
            "final_train_loss": round(train_losses[-1], 4),
            "final_train_acc": round(train_accs[-1], 4),
            "final_val_loss": round(val_losses[-1], 4),
            "final_val_acc": round(val_accs[-1], 4),
        },
        "metrics": final_metrics,
    }

    results_path = os.path.join(config.RESULTS_DIR, f"results_{config.SUBJECT}.json")
    save_results(results, results_path)

    print("\n" + "=" * 50)
    print("All done! ğŸ‰")
    print("=" * 50)
    print(f"Checkpoint: {checkpoint_path}")
    print(f"Results: {results_path}")
    print(f"Confusion Matrix: {cm_path}")
    print("=" * 50)


def test_main():
    """
    æµ‹è¯•ä¸»å‡½æ•° - ä½¿ç”¨ç”Ÿæˆçš„å‡æ•°æ®å¿«é€ŸéªŒè¯ä»£ç é€»è¾‘
    ä¸åŠ è½½çœŸå®æ•°æ®ï¼Œåªæµ‹è¯•æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹
    """
    print("\n" + "=" * 50)
    print("TEST MODE - Quick validation with synthetic data")
    print("=" * 50)

    # 1. ç”Ÿæˆå‡æ•°æ®
    print("\n[1/5] Generating synthetic data...")
    batch_size = 16
    n_samples = 100

    # ç”Ÿæˆéšæœºæ•°æ®ï¼š(n_samples, n_channels, 32, 32)
    fake_data = torch.randn(n_samples, config.N_CHANNELS, 32, 32)
    fake_labels = torch.randint(0, 2, (n_samples,))  # äºŒåˆ†ç±»æ ‡ç­¾

    # åˆ›å»ºç®€å•çš„TensorDataset
    from torch.utils.data import TensorDataset

    fake_dataset = TensorDataset(fake_data, fake_labels)

    # åˆ†å‰²æ•°æ®é›†
    train_size = int(0.6 * n_samples)
    val_size = int(0.2 * n_samples)
    test_size = n_samples - train_size - val_size

    train_data = TensorDataset(fake_data[:train_size], fake_labels[:train_size])
    val_data = TensorDataset(
        fake_data[train_size : train_size + val_size],
        fake_labels[train_size : train_size + val_size],
    )
    test_data = TensorDataset(
        fake_data[train_size + val_size :], fake_labels[train_size + val_size :]
    )

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

    print(f"Train samples: {len(train_data)}")
    print(f"Val samples: {len(val_data)}")
    print(f"Test samples: {len(test_data)}")

    # 2. åˆ›å»ºæ¨¡å‹
    print("\n[2/5] Building model...")
    model = MultiChannelViT(
        n_channels=config.N_CHANNELS,
        patch_size=config.PATCH_SIZE,
        embed_dim=config.EMBED_DIM,
        num_layers=config.NUM_LAYERS,
        num_heads=config.NUM_HEADS,
        mlp_dim=config.MLP_DIM,
        num_classes=config.NUM_CLASSES,
        dropout=config.DROPOUT,
    ).to(config.DEVICE)

    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # 3. æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n[3/5] Testing forward pass...")
    model.eval()
    with torch.no_grad():
        x, y = next(iter(train_loader))
        x, y = x.to(config.DEVICE), y.to(config.DEVICE)
        output = model(x)
        print(f"Input shape: {x.shape}")
        print(f"Output shape: {output.shape}")
        print(f"Labels shape: {y.shape}")
        print("âœ“ Forward pass successful")

    # 4. è®­ç»ƒæµ‹è¯•
    print("\n[4/5] Training test (5 batches)...")
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)

    model.train()
    total_loss = 0
    for i, (inputs, labels) in enumerate(train_loader):
        if i >= 5:  # åªè®­ç»ƒ5ä¸ªbatch
            break

        inputs = inputs.to(config.DEVICE)
        labels = labels.to(config.DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        print(f"  Batch {i + 1}/5: Loss = {loss.item():.4f}")

    avg_loss = total_loss / min(5, len(train_loader))
    print(f"Average training loss: {avg_loss:.4f}")
    print("âœ“ Training loop successful")

    # 5. æµ‹è¯•è¯„ä¼°
    print("\n[5/5] Testing evaluation...")
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(config.DEVICE)
            labels = labels.to(config.DEVICE)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))
    print_metrics(metrics)
    print("âœ“ Evaluation successful")

    # è®¡ç®—æŒ‡æ ‡
    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))
    print_metrics(metrics)

    print("\n" + "=" * 50)
    print("Test completed successfully! âœ“")
    print("You can now run the full training with main()")
    print("=" * 50)


if __name__ == "__main__":
    import sys

    # å¦‚æœå‘½ä»¤è¡Œå‚æ•°åŒ…å« --testï¼Œè¿è¡Œæµ‹è¯•æ¨¡å¼
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        test_main()
    else:
        main()
